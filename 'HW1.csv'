{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc23a80-4eee-46db-b508-3ee5f27f783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem 1.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('HW1.csv')\n",
    "\n",
    "# Extracting variables\n",
    "X1 = df['x1'].values\n",
    "X2 = df['x2'].values\n",
    "X3 = df['x3'].values\n",
    "Y = df['Y'].values\n",
    "\n",
    "# Function to perform gradient descent\n",
    "def gradient_descent(X, Y, learning_rate, n_iterations):\n",
    "    m = len(Y)\n",
    "    theta = 0  # Initialize parameter to zero\n",
    "    history = []  # To record the loss history\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        prediction = theta * X\n",
    "        error = prediction - Y\n",
    "        cost = (1 / (2 * m)) * np.sum(error ** 2)\n",
    "        gradient = (1 / m) * np.sum(error * X)\n",
    "        theta = theta - learning_rate * gradient\n",
    "        history.append(cost)\n",
    "        \n",
    "    return theta, history\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rates = [0.1, 0.05, 0.01]\n",
    "n_iterations = 1000\n",
    "\n",
    "# Initialize dictionaries to store results\n",
    "results = {}\n",
    "\n",
    "# Perform gradient descent for each variable with different learning rates\n",
    "for i, (X, var_name) in enumerate(zip([X1, X2, X3], ['X1', 'X2', 'X3']), start=1):\n",
    "    for lr in learning_rates:\n",
    "        theta, history = gradient_descent(X, Y, lr, n_iterations)\n",
    "        results[f'{var_name}_lr{lr}'] = (theta, history)\n",
    "\n",
    "# Plotting the final regression model and loss over iterations\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "for key, (theta, history) in results.items():\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(history, label=key)\n",
    "    plt.title('Loss over Iterations')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.scatter(df[key.split('_')[0]], Y, label='Actual data')\n",
    "    plt.plot(df[key.split('_')[0]], theta * df[key.split('_')[0]], color='red', label='Regression line')\n",
    "    plt.title(f'Regression Model: {key}')\n",
    "    plt.xlabel(key.split('_')[0])\n",
    "    plt.ylabel('Y')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Finding which variable has the lowest loss\n",
    "lowest_loss = float('inf')\n",
    "best_variable = None\n",
    "\n",
    "for key, (theta, history) in results.items():\n",
    "    final_loss = history[-1]\n",
    "    if final_loss < lowest_loss:\n",
    "        lowest_loss = final_loss\n",
    "        best_variable = key\n",
    "\n",
    "print(f'Variable with the lowest loss: {best_variable}, Final Loss: {lowest_loss}')\n",
    "\n",
    "# Analysis of learning rates impact on final loss and convergence\n",
    "for key, (theta, history) in results.items():\n",
    "    print(f'\\nVariable: {key}, Final Theta: {theta}, Final Loss: {history[-1]}')\n",
    "    for lr in learning_rates:\n",
    "        if key.endswith(f'lr{lr}'):\n",
    "            print(f'Learning Rate {lr}: Number of Iterations: {len(history)}, Final Loss: {history[-1]}')\n",
    "\n",
    "# Save the results and plots in a PDF\n",
    "plt.figure(figsize=(14, 10))\n",
    "for key, (theta, history) in results.items():\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(history, label=key)\n",
    "    plt.title('Loss over Iterations')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.scatter(df[key.split('_')[0]], Y, label='Actual data')\n",
    "    plt.plot(df[key.split('_')[0]], theta * df[key.split('_')[0]], color='red', label='Regression line')\n",
    "    plt.title(f'Regression Model: {key}')\n",
    "    plt.xlabel(key.split('_')[0])\n",
    "    plt.ylabel('Y')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('HW1_results.pdf')\n",
    "plt.close()\n",
    "\n",
    "# Save the code and results in a PDF\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "with PdfPages('HW1_code_and_results.pdf') as pdf:\n",
    "    pdf.savefig()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Problem 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('HW1.csv')\n",
    "\n",
    "# Extracting variables\n",
    "X1 = df['x1'].values\n",
    "X2 = df['x2'].values\n",
    "X3 = df['x3'].values\n",
    "Y = df['Y'].values\n",
    "\n",
    "# Function to perform gradient descent with multiple variables (X1, X2, X3)\n",
    "def gradient_descent_multi(X, Y, learning_rate, n_iterations):\n",
    "    m = len(Y)\n",
    "    n = X.shape[1]  # number of features\n",
    "    theta = np.zeros(n)  # Initialize parameters to zero\n",
    "    history = []  # To record the loss history\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        prediction = np.dot(X, theta)\n",
    "        error = prediction - Y\n",
    "        cost = (1 / (2 * m)) * np.sum(error ** 2)\n",
    "        gradient = (1 / m) * np.dot(X.T, error)\n",
    "        theta = theta - learning_rate * gradient\n",
    "        history.append(cost)\n",
    "        \n",
    "    return theta, history\n",
    "\n",
    "# Combine X1, X2, X3 into a feature matrix X\n",
    "X = np.column_stack((X1, X2, X3))\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rates = [0.1, 0.05, 0.01]\n",
    "n_iterations = 1000\n",
    "\n",
    "# Initialize dictionaries to store results\n",
    "results_multi = {}\n",
    "\n",
    "# Perform gradient descent for each learning rate\n",
    "for lr in learning_rates:\n",
    "    theta, history = gradient_descent_multi(X, Y, lr, n_iterations)\n",
    "    results_multi[f'LR_{lr}'] = (theta, history)\n",
    "\n",
    "# Plotting the loss over iterations for each learning rate\n",
    "plt.figure(figsize=(10, 6))\n",
    "for key, (theta, history) in results_multi.items():\n",
    "    plt.plot(history, label=f'Learning Rate {key.split(\"_\")[1]}')\n",
    "plt.title('Loss over Iterations (Multi-variable Gradient Descent)')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find the best model based on final loss\n",
    "best_lr = None\n",
    "best_theta = None\n",
    "lowest_loss = float('inf')\n",
    "\n",
    "for key, (theta, history) in results_multi.items():\n",
    "    final_loss = history[-1]\n",
    "    if final_loss < lowest_loss:\n",
    "        lowest_loss = final_loss\n",
    "        best_lr = key\n",
    "        best_theta = theta\n",
    "\n",
    "print(f'Best learning rate: {best_lr.split(\"_\")[1]}')\n",
    "print(f'Final Theta (Parameters): {best_theta}')\n",
    "print(f'Final Loss: {lowest_loss}')\n",
    "\n",
    "# Predicting new values\n",
    "def predict(X_new, theta):\n",
    "    X_new = np.array(X_new)\n",
    "    if X_new.ndim == 1:  # If only one sample is given\n",
    "        X_new = np.insert(X_new, 0, 1)  # Insert 1 for intercept term\n",
    "        return np.dot(X_new, theta)\n",
    "    else:  # If multiple samples are given\n",
    "        m = X_new.shape[0]\n",
    "        X_new = np.insert(X_new, 0, np.ones(m), axis=1)  # Insert column of 1s for intercept term\n",
    "        return np.dot(X_new, theta)\n",
    "\n",
    "# Predicting Y for new (X1, X2, X3) values\n",
    "new_data = [(1, 1, 1), (2, 0, 4), (3, 2, 1)]\n",
    "for data in new_data:\n",
    "    prediction = predict(data, best_theta)\n",
    "    print(f'For (X1, X2, X3) = {data}, Predicted Y = {prediction}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
